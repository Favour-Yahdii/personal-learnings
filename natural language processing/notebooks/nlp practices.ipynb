{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: dobmeos with hgh my energy level has gone up! Stukm\n",
      "Introducing\n",
      "Doctor - formulated\n",
      "Hgh\n",
      "Human growth hormone - also called hgh\n",
      "Is referred to in medical science as the master hormone. It is very plentiful\n",
      "When we are young, but near the age of twenty - one our bodies begin to produce\n",
      "Less of it. By the time we are forty nearly everyone is deficient in hgh,\n",
      "And at eighty our production has normally diminished at least 90 - 95%.\n",
      "Advantages of hgh:\n",
      "- increased muscle strength\n",
      "- loss in body fat\n",
      "- increased bone density\n",
      "- lower blood pressure\n",
      "- quickens wound healing\n",
      "- reduces cellulite\n",
      "- improved vision\n",
      "- wrinkle disappearance\n",
      "- increased skin thickness texture\n",
      "- increased energy levels\n",
      "- improved sleep and emotional stability\n",
      "- improved memory and mental alertness\n",
      "- increased sexual potency\n",
      "- resistance to common illness\n",
      "- strengthened heart muscle\n",
      "- controlled cholesterol\n",
      "- controlled mood swings\n",
      "- new hair growth and color restore\n",
      "Read\n",
      "More at this website\n",
      "Unsubscribe\n",
      "\n",
      "Subject: christmas tree farm pictures\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def read_in(folder):\n",
    "    files = os.listdir(folder)\n",
    "    file_list = []\n",
    "    for f in files:\n",
    "        if not f.startswith('.'):\n",
    "            file_ = codecs.open(folder + '/' + f, 'r',\n",
    "                                encoding='ISO-8859-1', errors='ignore')\n",
    "            file_list.append(file_.read())\n",
    "            file_.close()\n",
    "    return file_list\n",
    "spam = read_in(r'C:\\Users\\Yahdii\\OneDrive\\Desktop\\personal learnings\\natural language processing\\data\\raw\\enron1\\enron1\\spam')\n",
    "ham = read_in(r'C:\\Users\\Yahdii\\OneDrive\\Desktop\\personal learnings\\natural language processing\\data\\raw\\enron1\\enron1\\ham')  \n",
    "print(spam[0])\n",
    "print(ham[0])   \n",
    "       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5172\n"
     ]
    }
   ],
   "source": [
    "# 2. Preprocessing\n",
    "import random\n",
    "random.seed(42)\n",
    "all_emails = [(email, 'spam') for email in spam]\n",
    "all_emails.extend([(email, 'ham') for email in ham])\n",
    "random.shuffle(all_emails)\n",
    "print(len(all_emails))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'am', 'learning', 'nlp'], ['ham']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Yahdii\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#3. split into word tokens\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "nltk.download('punkt') #nltk sentence tokenizer\n",
    "\n",
    "def tokenize(email):\n",
    "    word_list = [word_tokenize(word.lower()) for word in email]\n",
    "    return word_list\n",
    "\n",
    "input = ('I am learning NLP', 'ham')\n",
    "print(tokenize(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'subject': True, ':': True, 'bloodline': True, ',': True, 'ahead': True, 'of': True, 'the': True, 'street': True, 'microcap': True, 'alert': True, 'when': True, 'living': True, 'with': True, 'sheriff': True, 'is': True, 'obsequious': True, 'blood': True, 'clot': True, 'beyond': True, 'deficit': True, 'reach': True, 'an': True, 'understanding': True, 'toward': True, '.': True, '[': True, '3': True}, 'spam')\n"
     ]
    }
   ],
   "source": [
    "#feature extraction\n",
    "#we want a dictionary of words in the email\n",
    "def features(email):\n",
    "    feat = {word: True for word in word_tokenize(email.lower())}\n",
    "    return feat\n",
    "\n",
    "#all_eils = ('I am learning NLP', 'ham')\n",
    "all_features = [(features(email), label) for (email, label) in all_emails]\n",
    "print(all_features[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "naturallang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
